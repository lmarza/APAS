{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Experiments ECAI24 on the Diabetes dataset\n",
    "\n",
    "8 input features, all continuous. No known distribution shifts in this dataset, so it is used to mimic the daily model updates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# sci-kit learn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from util_scripts.convert import extract_sklearn_params, custom_nn_model\n",
    "from joblib import dump, load\n",
    "import gurobipy\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 150\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from util_scripts.preprocessor import Preprocessor, min_max_scale\n",
    "from util_scripts.utilexp import *\n",
    "from interval import *\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "CYAN_COL = '\\033[96m'\n",
    "BLUE_COL = '\\033[94m'\n",
    "RED_COL = '\\033[91m'\n",
    "GREEN_COL = '\\033[92m'\n",
    "YELLOW_COL = '\\033[93m'\n",
    "RESET_COL = '\\033[0m'\n",
    "BOLD = '\\033[1m'\n",
    "UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./datasets/diabetes/diabetes.csv\")\n",
    "df = df.dropna()\n",
    "display(df)\n",
    "ordinal_features = {}\n",
    "discrete_features = {}\n",
    "continuous_features = list(df.columns)[:-1]\n",
    "#columns = copy.deepcopy(continuous_features)\n",
    "CLASS = \"Outcome\"\n",
    "\n",
    "# min max scale\n",
    "min_vals = np.min(df[continuous_features], axis=0)\n",
    "max_vals = np.max(df[continuous_features], axis=0)\n",
    "df_mm = min_max_scale(df, continuous_features, min_vals, max_vals)\n",
    "columns = list(df_mm.columns)\n",
    "# get X, y\n",
    "X, y = df_mm.drop(columns=['Outcome']), pd.DataFrame(df_mm['Outcome'])\n",
    "\n",
    "SPLIT = .2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=SPLIT, shuffle=True,\n",
    "                                                    random_state=0)\n",
    "feat_var_map = {}\n",
    "for i in range(len(X.columns)):\n",
    "    feat_var_map[i] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx_1 = np.sort(np.random.choice(range(768), 384))\n",
    "idx_2 = np.array([i for i in range(768) if i not in idx_1])\n",
    "\n",
    "X1 = pd.DataFrame(data=X.values[idx_1], columns=X.columns)\n",
    "y1 = pd.DataFrame(data=y.values[idx_1], columns=y.columns)\n",
    "X2 = pd.DataFrame(data=X.values[idx_2], columns=X.columns)\n",
    "y2 = pd.DataFrame(data=y.values[idx_2], columns=y.columns)\n",
    "\n",
    "SPLIT = .2\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, stratify=y1, test_size=SPLIT, shuffle=True,\n",
    "                                                        random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train models and Observe model shifts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Randomdised search + 5-fold cross validation (default)\n",
    "nn = MLPClassifier(learning_rate='adaptive', random_state=0)\n",
    "\n",
    "# parameters\n",
    "max_iter_vals = [int(i) for i in np.linspace(1000, 10000, 10)]\n",
    "hidden_layer_sizes_vals = [(i) for i in range(5, 16)]\n",
    "batch_size_vals = [8, 16, 32, 64]\n",
    "learning_rate_init_vals = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05]\n",
    "\n",
    "#distributions = dict(max_iter=max_iter_vals, hidden_layer_sizes=hidden_layer_sizes_vals)\n",
    "distributions = dict(hidden_layer_sizes=hidden_layer_sizes_vals,\n",
    "                     batch_size=batch_size_vals,\n",
    "                     learning_rate_init=learning_rate_init_vals,\n",
    "                     max_iter=max_iter_vals, )\n",
    "\n",
    "#nns = RandomizedSearchCV(nn, distributions, scoring='f1_macro')\n",
    "#nns = RandomizedSearchCV(nn, distributions, scoring='accuracy')\n",
    "#search = nns.fit(X, y)\n",
    "#print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76 test_accuracy with a std of 0.04\n",
      "0.76 test_precision_macro with a std of 0.04\n",
      "0.72 test_recall_macro with a std of 0.03\n",
      "0.73 test_f1_macro with a std of 0.04\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " bad credit (0)      0.815     0.880     0.846        50\n",
      "good credit (1)      0.739     0.630     0.680        27\n",
      "\n",
      "       accuracy                          0.792        77\n",
      "      macro avg      0.777     0.755     0.763        77\n",
      "   weighted avg      0.788     0.792     0.788        77\n",
      "\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " bad credit (0)      0.849     0.863     0.856       248\n",
      "good credit (1)      0.742     0.721     0.731       136\n",
      "\n",
      "       accuracy                          0.812       384\n",
      "      macro avg      0.796     0.792     0.794       384\n",
      "   weighted avg      0.811     0.812     0.812       384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(learning_rate='adaptive', hidden_layer_sizes=8, learning_rate_init=0.01, batch_size=8,\n",
    "                    max_iter=7000, random_state=0)\n",
    "\n",
    "# 5-fold cross validation\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "scores = cross_validate(clf, X, y, scoring=scoring)\n",
    "for name in list(scores.keys()):\n",
    "    if name == 'fit_time' or name == 'score_time':\n",
    "        continue\n",
    "    print(\"%0.2f %s with a std of %0.2f\" % (scores[name].mean(), name, scores[name].std()))\n",
    "\n",
    "clf = MLPClassifier(learning_rate='adaptive', hidden_layer_sizes=8, learning_rate_init=0.01, batch_size=8,\n",
    "                    max_iter=7000, random_state=0)\n",
    "\n",
    "clf.fit(X1_train, y1_train)\n",
    "resres = clf.predict(X1_test.values)\n",
    "print('\\n', classification_report(y1_test, resres, target_names=[f'bad credit (0)', f'good credit (1)'], digits=3))\n",
    "resres = clf.predict(X1.values)\n",
    "print('\\n', classification_report(y1, resres, target_names=[f'bad credit (0)', f'good credit (1)'], digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/diabetes.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(clf, './models/diabetes.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Computing counterfactuals\n",
    "\n",
    "#### Procedures\n",
    "\n",
    "These procedures are covered by UtilExp class\n",
    "\n",
    "1. Train M on D1\n",
    "2. Get delta-min, build M+ and M-: incrementally train M 5 times, using different 10% of D2 each time, then get the maximum inf-distance between the incremented models and M. Construct M+ and M- using delta-min\n",
    "3. Get M2: incrementally train M on D2\n",
    "4. Select test instances: randomly select 50 D1 instances to explain, clf(x)=0, desired class=1\n",
    "5. Report metrics using each baseline\n",
    "\n",
    "#### Metrics\n",
    "- Proximity: normalised L1: \"Scaling Guarantees for Nearest CEs\" page 7\n",
    "- Sparsity: L0\n",
    "- Validity-delta: percentage of test instances that 1) have counterfactuals valid on m1, 2) counterfactuals valid on M+ and M- under delta_min\n",
    "- Validity-m2: percentage of test instances that 1) have counterfactual(s), 2) these counterfactual(s) are all valid on both m1 and m2\n",
    "- LOF: average LOF score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2025-11-24\n",
      "Set parameter FeasibilityTol to value 0.001\n",
      "Set parameter OptimalityTol to value 0.001\n",
      "Set parameter IntFeasTol to value 0.001\n"
     ]
    }
   ],
   "source": [
    "clf = load(\"./models/diabetes.joblib\")\n",
    "gurobipy.setParam(\"FeasibilityTol\", 1e-03) #1e-09\n",
    "gurobipy.setParam(\"OptimalityTol\", 1e-03) #1e-09\n",
    "gurobipy.setParam(\"IntFeasTol\", 1e-03) #1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3457906217784634\n",
      "0.1058395646526038\n"
     ]
    }
   ],
   "source": [
    "util_exp = UtilExp(clf, X1, y1, X2, y2, columns, ordinal_features, discrete_features, continuous_features, feat_var_map, num_test_instances=500, gap=0.25)\n",
    "print(util_exp.delta_max)\n",
    "print(util_exp.delta_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save model trained on the whole dataset\n",
    "m2 = copy.deepcopy(clf)\n",
    "m2.partial_fit(X2, y2)\n",
    "util_exp.Mmax = m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of sound model changes: 0.27380952380952384\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "# pre-verification on the points soundness \n",
    "valids = util_exp.verify_soundness()\n",
    "print(len(valids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of sound model changes: 0.27380952380952384\n",
      "test instances updated to sound (x, Delta) pairs, length: 50\n"
     ]
    }
   ],
   "source": [
    "valids = util_exp.verify_soundness(update_test_instances=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the $\\delta$ difference between m1 and m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=h5\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=h5\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum distance between weights is: 0.27983832\n"
     ]
    }
   ],
   "source": [
    "input_size, n_layers, output_size, output_act, h_act, optimizer, params = extract_sklearn_params(clf)\n",
    "tf_model = custom_nn_model(input_size, n_layers, params, output_size, h_act, output_act, optimizer)\n",
    "     \n",
    "# Set model weights\n",
    "for k, v in params.items():\n",
    "\ttf_model.layers[k].set_weights(v)\n",
    "        \n",
    "tf_model.save('./models/diabetes.h5', save_format='h5')   \n",
    "\n",
    "input_size, n_layers, output_size, output_act, h_act, optimizer, params = extract_sklearn_params(m2)\n",
    "tf_model = custom_nn_model(input_size, n_layers, params, output_size, h_act, output_act, optimizer)\n",
    "     \n",
    "# Set model weights\n",
    "for k, v in params.items():\n",
    "\ttf_model.layers[k].set_weights(v)\n",
    "\t\n",
    "tf_model.save('./models/diabetes_retrained.h5', save_format='h5')  \n",
    "\n",
    "original_model = tf.keras.models.load_model('./models/diabetes.h5', compile=False)\n",
    "old_weights = {}\n",
    "for l in range(1,len(original_model.layers)):\n",
    "\told_weights[l] = original_model.layers[l].get_weights()\n",
    "\n",
    "\n",
    "model_retrained = tf.keras.models.load_model('./models/diabetes_retrained.h5', compile=False)\n",
    "retrained_weights = {}\n",
    "for l in range(1,len(model_retrained.layers)):\n",
    "\tretrained_weights[l] = model_retrained.layers[l].get_weights()\n",
    "\n",
    "\n",
    "max_diff = -1\n",
    "for l in range(1,len(old_weights)):\n",
    "\told_layer_weights = old_weights[l][0]\n",
    "\tnew_retrained_weights = retrained_weights[l][0]\n",
    "\n",
    "\tdifference = abs(old_layer_weights - new_retrained_weights)\n",
    "\t\n",
    "\tfor list_weights in difference:\n",
    "\t\tmax_distance = max(list_weights)\n",
    "\t\tif max_distance > max_diff:\n",
    "\t\t\tmax_diff = max_distance\n",
    "\n",
    "print(\"The maximum distance between weights is:\", max_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CFX computation: in the following cells we both compute the CFX using MILP and the proposed probabilistic APΔS approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:11,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total computation time in s: 11.774839162826538\n",
      "found: 1.0\n",
      "average normalised L1: 0.07657846284575133\n",
      "average normalised L0: 0.3075\n",
      "average lof score: 1.0\n",
      "counterfactual validity: 1.0\n",
      "delta validity: 0.0\n",
      "m2 validity: 1.0\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# OURS-ROBUST: compute CFX based on the n sound points discovered above\n",
    "ours_robust_ces_apas = util_exp.run_ours_robust(approx=True)\n",
    "util_exp.evaluate_ces(ours_robust_ces_apas)\n",
    "cfxs_robust_apas = ours_robust_ces_apas\n",
    "print(len(cfxs_robust_apas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Robust CFXs using MILP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:06,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total computation time in s: 6.095015048980713\n",
      "found: 1.0\n",
      "average normalised L1: 0.21184376391537593\n",
      "average normalised L0: 0.415\n",
      "average lof score: -0.48\n",
      "counterfactual validity: 1.0\n",
      "delta validity: 1.0\n",
      "m2 validity: 1.0\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# OURS-ROBUST: compute CFX based on the n sound points discovered above\n",
    "ours_robust_ces = util_exp.run_ours_robust()\n",
    "util_exp.evaluate_ces(ours_robust_ces)\n",
    "cfxs_robust = ours_robust_ces\n",
    "print(len(cfxs_robust))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing how many robust CFX remains robust after the retraining (i.e., expanding the $\\delta$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage CFXs robust after the retraing: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# double checking the robustness of the CFXs found by APΔS approach after retraining\n",
    "tot_robust_cfx = len(cfxs_robust_apas)\n",
    "robust_cfx_after_retrain = 0\n",
    "\n",
    "for cfx in cfxs_robust_apas:\n",
    "    if model_retrained(np.array(cfx.reshape(1,-1))) >= 0.5:\n",
    "        robust_cfx_after_retrain += 1\n",
    "\n",
    "print(f\"Percentage CFXs robust after the retraing: {(robust_cfx_after_retrain/tot_robust_cfx)*100}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage CFXs robust after the retraing: 100.0%\n"
     ]
    }
   ],
   "source": [
    "tot_robust_cfx = len(cfxs_robust)\n",
    "robust_cfx_after_retrain = 0\n",
    "\n",
    "for cfx in cfxs_robust:\n",
    "    if model_retrained(np.array(cfx.reshape(1,-1))) >= 0.5:\n",
    "        robust_cfx_after_retrain += 1\n",
    "\n",
    "print(f\"Percentage CFXs robust after the retraing: {(robust_cfx_after_retrain/tot_robust_cfx)*100}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: probabilistic approximation approach to compute $\\Delta$-robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_robustness(model, delta, cfx, concretizations, use_biases=True, robustness=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Utility method for the estimation of the CFX (not) Δ-robustness in the INN.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        rate: float\n",
    "            estimation of the CFX (not) Δ-robustness computed with 'concretizations' models concretizations from the INN\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Store initial weights\n",
    "    old_weights = {}\n",
    "    for l in range(1,len(model.layers)):\n",
    "        old_weights[l] = model.layers[l].get_weights()\n",
    "\n",
    "    for _ in range(concretizations):\n",
    "        \n",
    "        #perturbated_weights = {}\n",
    "        input_features = np.array(cfx)\n",
    "\n",
    "        for l in range(1,len(old_weights)+1):\n",
    "            layer_weights = old_weights[l][0]\n",
    "            if use_biases: layer_biases  = old_weights[l][1]\n",
    "            \n",
    "            weights_perturbation = np.random.uniform(-delta, delta, layer_weights.shape)\n",
    "            if use_biases: biases_perturbation = np.random.uniform(-delta, delta, layer_biases.shape)\n",
    "           \n",
    "            \n",
    "            layer_weights = [layer_weights+weights_perturbation]\n",
    "\n",
    "            if use_biases: \n",
    "                layer_biases = [layer_biases+biases_perturbation]\n",
    "                preactivated_res = np.dot(input_features, layer_weights) + layer_biases\n",
    "            else:\n",
    "                preactivated_res = np.dot(input_features, layer_weights)\n",
    "\n",
    "            if l != len(old_weights):\n",
    "                #relu\n",
    "                activated_res = np.maximum(0.0, preactivated_res)\n",
    "            else:\n",
    "                #sigmoid\n",
    "                activated_res = 1/(1 + np.exp(-preactivated_res))\n",
    "            \n",
    "            input_features = activated_res\n",
    "            \n",
    "        if input_features < 0.5:\n",
    "            return 0  \n",
    "    \n",
    "    return 1\n",
    "\n",
    "def compute_delta_max_MILP(cfx, delta_init, verbose=False):\n",
    "  \n",
    "    lower = 1/(1 + np.exp(-util_exp.is_robust_custom_delta_new(cfx, delta_init)))\n",
    "    if lower < 0.5: return 0 # CFX not robust\n",
    "        \n",
    "    delta = delta_init\n",
    "    while lower >= 0.5: # over-approx lower bound is >= 0.5, i.e., x results robust\n",
    "        delta = 2*delta\n",
    "        lower = 1/(1 + np.exp(-util_exp.is_robust_custom_delta_new(cfx, delta)))\n",
    "        if verbose: \n",
    "            print(f'Testing δ={delta}')\n",
    "            print(f'Lower is: {lower}')\n",
    "    \n",
    "    delta_max = delta/2\n",
    "    \n",
    "    while True:\n",
    "        if abs(delta-delta_max) < delta_init:\n",
    "            return delta_max\n",
    "\n",
    "        if verbose: print(f\"\\nInterval to test is: [{delta_max}, {delta}]\")\n",
    "        \n",
    "        delta_new = (delta_max+delta)/2\n",
    "        lower = 1/(1 + np.exp(-util_exp.is_robust_custom_delta_new(cfx, delta_new)))\n",
    "        if verbose: \n",
    "            print(f'Testing δ={delta_new}')\n",
    "            print(f'Rate is: {lower}')\n",
    "        \n",
    "        if lower >= 0.5:\n",
    "            delta_max = delta_new\n",
    "        else:\n",
    "            delta = delta_new\n",
    "\n",
    "\n",
    "\n",
    "def compute_delta_max(model, cfx, delta_init, concretizations, use_biases=True, verbose=False):\n",
    "  \n",
    "    rate = estimate_robustness(model, delta_init, cfx, concretizations, use_biases)\n",
    "    if rate != 1: return 0 # CFX not robust\n",
    "        \n",
    "    delta = delta_init\n",
    "    while rate == 1: # for all the concretizations x results robust\n",
    "        delta = 2*delta\n",
    "        rate = estimate_robustness(model, delta, cfx, concretizations, use_biases)\n",
    "        if verbose: \n",
    "            print(f'Testing δ={delta}')\n",
    "            print(f'Rate is: {rate}')\n",
    "    \n",
    "    delta_max = delta/2\n",
    "    \n",
    "    while True:\n",
    "        if abs(delta-delta_max) < delta_init:\n",
    "            return delta_max\n",
    "\n",
    "        if verbose: print(f\"\\nInterval to test is: [{delta_max}, {delta}]\")\n",
    "        \n",
    "        delta_new = (delta_max+delta)/2\n",
    "        rate = estimate_robustness(model, delta_new, cfx, concretizations, use_biases)\n",
    "        if verbose: \n",
    "            print(f'Testing δ={delta_new}')\n",
    "            print(f'Rate is: {rate}')\n",
    "        \n",
    "        if rate == 1:\n",
    "            delta_max = delta_new\n",
    "        else:\n",
    "            delta = delta_new\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96mCondifence α =(1-R^n)=99.89995272421471%, R=99.5%, Concretizations(n)=1378\u001b[0m\n",
      "δ_max = 0.2855000000000001\n",
      "δ improvement w.r.t original MILP's δ is 0.1741000000000001\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.28390000000000004\n",
      "δ improvement w.r.t original MILP's δ is 0.17660000000000003\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.26470000000000005\n",
      "δ improvement w.r.t original MILP's δ is 0.15570000000000003\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2743\n",
      "δ improvement w.r.t original MILP's δ is 0.16549999999999998\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2569\n",
      "δ improvement w.r.t original MILP's δ is 0.1484\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.28390000000000004\n",
      "δ improvement w.r.t original MILP's δ is 0.17660000000000003\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2872\n",
      "δ improvement w.r.t original MILP's δ is 0.1809\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2753\n",
      "δ improvement w.r.t original MILP's δ is 0.16765\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2713\n",
      "δ improvement w.r.t original MILP's δ is 0.16409999999999997\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.25570000000000004\n",
      "δ improvement w.r.t original MILP's δ is 0.14875000000000002\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2855000000000001\n",
      "δ improvement w.r.t original MILP's δ is 0.1741000000000001\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.27669999999999995\n",
      "δ improvement w.r.t original MILP's δ is 0.16834999999999994\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.26070000000000004\n",
      "δ improvement w.r.t original MILP's δ is 0.15475000000000005\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.26460000000000006\n",
      "δ improvement w.r.t original MILP's δ is 0.15720000000000006\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2855000000000001\n",
      "δ improvement w.r.t original MILP's δ is 0.1741000000000001\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2754\n",
      "δ improvement w.r.t original MILP's δ is 0.16584999999999997\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.28390000000000004\n",
      "δ improvement w.r.t original MILP's δ is 0.17660000000000003\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.27580000000000005\n",
      "δ improvement w.r.t original MILP's δ is 0.16750000000000004\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.27859999999999996\n",
      "δ improvement w.r.t original MILP's δ is 0.16859999999999997\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2713\n",
      "δ improvement w.r.t original MILP's δ is 0.16429999999999997\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.27485000000000004\n",
      "δ improvement w.r.t original MILP's δ is 0.16860000000000006\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2935\n",
      "δ improvement w.r.t original MILP's δ is 0.18344999999999997\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2772\n",
      "δ improvement w.r.t original MILP's δ is 0.1663\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.20740000000000003\n",
      "δ improvement w.r.t original MILP's δ is 0.09335000000000002\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2935\n",
      "δ improvement w.r.t original MILP's δ is 0.18344999999999997\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.26450000000000007\n",
      "δ improvement w.r.t original MILP's δ is 0.15535000000000007\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2614000000000001\n",
      "δ improvement w.r.t original MILP's δ is 0.15330000000000005\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2663000000000001\n",
      "δ improvement w.r.t original MILP's δ is 0.15625000000000008\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.26870000000000005\n",
      "δ improvement w.r.t original MILP's δ is 0.16075000000000003\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.26460000000000006\n",
      "δ improvement w.r.t original MILP's δ is 0.15720000000000006\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2569\n",
      "δ improvement w.r.t original MILP's δ is 0.1484\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.27859999999999996\n",
      "δ improvement w.r.t original MILP's δ is 0.16859999999999997\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.27485000000000004\n",
      "δ improvement w.r.t original MILP's δ is 0.16860000000000006\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.27669999999999995\n",
      "δ improvement w.r.t original MILP's δ is 0.16834999999999994\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.27485000000000004\n",
      "δ improvement w.r.t original MILP's δ is 0.16860000000000006\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2713\n",
      "δ improvement w.r.t original MILP's δ is 0.16429999999999997\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2872\n",
      "δ improvement w.r.t original MILP's δ is 0.1809\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2754\n",
      "δ improvement w.r.t original MILP's δ is 0.16584999999999997\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.273\n",
      "δ improvement w.r.t original MILP's δ is 0.16355000000000003\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.27669999999999995\n",
      "δ improvement w.r.t original MILP's δ is 0.16834999999999994\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2834\n",
      "δ improvement w.r.t original MILP's δ is 0.17329999999999998\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.26870000000000005\n",
      "δ improvement w.r.t original MILP's δ is 0.16075000000000003\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2855000000000001\n",
      "δ improvement w.r.t original MILP's δ is 0.1741000000000001\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.26770000000000005\n",
      "δ improvement w.r.t original MILP's δ is 0.15980000000000005\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.27859999999999996\n",
      "δ improvement w.r.t original MILP's δ is 0.16859999999999997\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2752\n",
      "δ improvement w.r.t original MILP's δ is 0.1631\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.28390000000000004\n",
      "δ improvement w.r.t original MILP's δ is 0.17660000000000003\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2751\n",
      "δ improvement w.r.t original MILP's δ is 0.16345\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2872\n",
      "δ improvement w.r.t original MILP's δ is 0.1809\n",
      "______________________________________________________________________________________\n",
      "δ_max = 0.2855000000000001\n",
      "δ improvement w.r.t original MILP's δ is 0.1741000000000001\n",
      "______________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./models/diabetes.h5', compile=False)\n",
    "alpha = 0.999\n",
    "R = 0.995\n",
    "concretizations = int(np.emath.logn(R, (1-alpha)))\n",
    "delta_init = 0.0001\n",
    "delta_AAAI = 0.1058395646526038\n",
    "\n",
    "with open(\"full_results_diabetes.csv\", mode='w', newline='') as file:\n",
    "\n",
    "    csv_writer = csv.writer(file)\n",
    "    csv_writer.writerow([\"CFX\", \"AAAI δ_max\", \"Wilks δ_max\", \"MILP δ_max\", \"Difference\"])\n",
    "\n",
    "    wilks = []\n",
    "    milp = []\n",
    "    print( f\"{CYAN_COL}Condifence α =(1-R^n)={(1-R**concretizations)*100}%, R={R*100}%, Concretizations(n)={concretizations}{RESET_COL}\")\n",
    "\n",
    "    # start computing the new deltas with the approximation\n",
    "    for cfx in cfxs_robust:\n",
    "\n",
    "        delta_max_sampling = compute_delta_max(model, cfx.reshape(1,-1), delta_init, concretizations,verbose=False)\n",
    "        delta_max_MILP = compute_delta_max_MILP(cfx, delta_init,verbose=False) \n",
    "        difference = abs(delta_max_sampling-delta_max_MILP)\n",
    "        wilks.append(delta_max_sampling)\n",
    "        milp.append(delta_max_MILP)\n",
    "        csv_writer.writerow([cfx,  delta_AAAI, delta_max_sampling, delta_max_MILP, difference])\n",
    "\n",
    "        print('δ_max =', delta_max_sampling)\n",
    "        print('δ improvement w.r.t original MILP\\'s δ is',difference)\n",
    "        print(\"______________________________________________________________________________________\")\n",
    "\n",
    "\n",
    "\n",
    "    csv_writer.writerow([\" \"])\n",
    "    csv_writer.writerow([\"Mean MILP\", \"Mean Wilks\"])\n",
    "    csv_writer.writerow([np.mean(np.array(milp)), np.mean(np.array(wilks))])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
