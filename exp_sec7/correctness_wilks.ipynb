{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments ECAI24 on the approximation correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from interval import *\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "CYAN_COL = '\\033[96m'\n",
    "BLUE_COL = '\\033[94m'\n",
    "RED_COL = '\\033[91m'\n",
    "GREEN_COL = '\\033[92m'\n",
    "YELLOW_COL = '\\033[93m'\n",
    "RESET_COL = '\\033[0m'\n",
    "BOLD = '\\033[1m'\n",
    "UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we test and confirm the correctness of our approximation. More specifically we consider this toy example to show that for a given $\\delta$ discovered by our approximation and for which a MILP solver returns a non-robust result, with a confidence $\\alpha$ could exist at most 1-R percentage of non-robust subINNs obtained from the original INN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"util_scripts/INN.png\" alt=\"Drawing\" align=\"center\" style=\"width: 600px;\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_robustness(model, delta, cfx, concretizations, use_biases=True, robustness=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Utility method for the estimation of the CFX (not) Δ-robustness in the INN.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        rate: float\n",
    "            estimation of the CFX (not) Δ-robustness computed with 'concretizations' models concretizations from the INN\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    # Store initial weights\n",
    "    old_weights = {}\n",
    "    for l in range(1,len(model.layers)):\n",
    "        old_weights[l] = model.layers[l].get_weights()\n",
    "\n",
    "    for _ in range(concretizations):\n",
    "        \n",
    "        #perturbated_weights = {}\n",
    "        input_features = np.array(cfx)\n",
    "\n",
    "        for l in range(1,len(old_weights)+1):\n",
    "            layer_weights = old_weights[l][0]\n",
    "            if use_biases: layer_biases  = old_weights[l][1]\n",
    "            \n",
    "            weights_perturbation = np.random.uniform(-delta, delta, layer_weights.shape)\n",
    "            if use_biases: biases_perturbation = np.random.uniform(-delta, delta, layer_biases.shape)\n",
    "           \n",
    "            \n",
    "            layer_weights = [layer_weights+weights_perturbation]\n",
    "\n",
    "            if use_biases: \n",
    "                layer_biases = [layer_biases+biases_perturbation]\n",
    "                preactivated_res = np.dot(input_features, layer_weights) + layer_biases\n",
    "            else:\n",
    "                preactivated_res = np.dot(input_features, layer_weights)\n",
    "\n",
    "            if l != len(old_weights):\n",
    "                #relu\n",
    "                activated_res = np.maximum(0.0, preactivated_res)\n",
    "            else:\n",
    "                #sigmoid\n",
    "                activated_res = 1/(1 + np.exp(-preactivated_res))\n",
    "            \n",
    "            input_features = activated_res\n",
    "            \n",
    "        if input_features < 0.5:\n",
    "            return 0  \n",
    "    \n",
    "    return 1\n",
    "\n",
    "def compute_delta_max(model, cfx, delta_init, concretizations, use_biases=True, verbose=False):\n",
    "  \n",
    "    rate = estimate_robustness(model, delta_init, cfx, concretizations, use_biases)\n",
    "    if rate != 1: return 0 # CFX not robust\n",
    "        \n",
    "    delta = delta_init\n",
    "    while rate == 1: # for all the concretizations x results robust\n",
    "        delta = 2*delta\n",
    "        rate = estimate_robustness(model, delta, cfx, concretizations, use_biases)\n",
    "        if verbose: \n",
    "            print(f'Testing δ={delta}')\n",
    "            print(f'Rate is: {rate}')\n",
    "    \n",
    "    delta_max = delta/2\n",
    "    \n",
    "    while True:\n",
    "        if abs(delta-delta_max) < delta_init:\n",
    "            return delta_max\n",
    "\n",
    "        if verbose: print(f\"\\nInterval to test is: [{delta_max}, {delta}]\")\n",
    "        \n",
    "        delta_new = (delta_max+delta)/2\n",
    "        rate = estimate_robustness(model, delta_new, cfx, concretizations, use_biases)\n",
    "        if verbose: \n",
    "            print(f'Testing δ={delta_new}')\n",
    "            print(f'Rate is: {rate}')\n",
    "        \n",
    "        if rate == 1:\n",
    "            delta_max = delta_new\n",
    "        else:\n",
    "            delta = delta_new\n",
    "\n",
    "\n",
    "def IBP(model, cfx, delta, use_biases=True):\n",
    "    # Store initial weights\n",
    "    old_weights = {}\n",
    "    for l in range(1,len(model.layers)):\n",
    "        old_weights[l] = model.layers[l].get_weights()\n",
    " \n",
    "    # create input bounds from CFX for IBP\n",
    "    input_bounds = []\n",
    "    for elem in cfx:\n",
    "        input_bounds.append(interval([elem, elem]))\n",
    "    \n",
    "\n",
    "    ### MAIN IBP LOOP\n",
    "    for layer in range(1,len(old_weights)+1):\n",
    "        layer_weights = old_weights[layer][0]\n",
    "        layer_biases  = old_weights[layer][1]\n",
    "\n",
    "        # every list in layer_weights is the list ok weights that exit from the i-th node of the previous layer. For instance if layer_weights is a list of 5 lists where each of the five list has 7 element, this means that the previous layer has 5 nodes, the next layer has 7 nodes, and the weights that start from the first node of the previous layer to the all the seven nodes of the next layer are stored in the first of the five lists.\n",
    "        # we have to create for each weight an interval and replace the scalar weight with the interval.\n",
    "        layer_weights_intervals = []\n",
    "        for weights_starting_from_node in layer_weights:\n",
    "            layer_intervals = []\n",
    "            for weight in weights_starting_from_node:\n",
    "                weight_interval = interval([weight-delta, weight+delta])\n",
    "                layer_intervals.append(weight_interval)\n",
    "            \n",
    "            layer_weights_intervals.append(layer_intervals)\n",
    "\n",
    "        if use_biases:\n",
    "            layer_biases_intervals = []\n",
    "            for i in range(len(layer_biases)):\n",
    "                layer_biases_intervals.append(interval([layer_biases[i]-delta, layer_biases[i]+delta]))\n",
    "\n",
    "      \n",
    "        # now in layer_weights_intervals we have the same structure described above but with an interval instead of a scalar weight. And we proceed to generate the linear combination using interval bound multiplication \n",
    "        new_layer_bounds = {}\n",
    "        index_input = -1\n",
    "        for weight_intervals in layer_weights_intervals:\n",
    "            index_input += 1\n",
    "            index_new_input = 0\n",
    "            for w_interval in weight_intervals:  \n",
    "                if index_new_input in new_layer_bounds:\n",
    "                    new_layer_bounds[index_new_input] += (w_interval*input_bounds[index_input])\n",
    "                else:\n",
    "                    new_layer_bounds[index_new_input] = (w_interval*input_bounds[index_input])\n",
    "\n",
    "                index_new_input += 1\n",
    "\n",
    "        if use_biases:\n",
    "            for i in range(len(new_layer_bounds)):\n",
    "                new_layer_bounds[i] += layer_biases_intervals[i]\n",
    "       \n",
    "        activated_bounds = []\n",
    "        if layer != len(old_weights):\n",
    "            #relu\n",
    "            for node in new_layer_bounds:\n",
    "                activated_bounds.append(interval(np.maximum(0.0, new_layer_bounds[node][0])))\n",
    "        else:\n",
    "            #sigmoid\n",
    "            for node in new_layer_bounds:\n",
    "                activated_bounds.append(1/(1 + np.exp(-new_layer_bounds[node][0][0])))\n",
    "\n",
    "        input_bounds = activated_bounds\n",
    "        #print(input_bounds)\n",
    "        \n",
    "\n",
    "    return input_bounds\n",
    "\n",
    "  \n",
    "\n",
    "def compute_formal_delta(model, cfx, delta_init, use_biases=True, verbose=False):\n",
    "  \n",
    "    lower = IBP(model, cfx, delta_init)[0]\n",
    "    if lower < 0.5: return 0 # CFX not robust\n",
    "        \n",
    "    delta = delta_init\n",
    "    while lower >= 0.5: # over-approx lower bound is >= 0.5, i.e., x results robust\n",
    "        delta = 2*delta\n",
    "        lower = IBP(model, cfx, delta, use_biases)[0]\n",
    "        if verbose: \n",
    "            print(f'Testing δ={delta}')\n",
    "            print(f'Lower is: {lower}')\n",
    "    \n",
    "    delta_max = delta/2\n",
    "    \n",
    "    while True:\n",
    "        if abs(delta-delta_max) < delta_init:\n",
    "            return delta_max\n",
    "\n",
    "        if verbose: print(f\"\\nInterval to test is: [{delta_max}, {delta}]\")\n",
    "        \n",
    "        delta_new = (delta_max+delta)/2\n",
    "        lower = IBP(model, cfx, delta_new, use_biases)[0]\n",
    "        if verbose: \n",
    "            print(f'Testing δ={delta_new}')\n",
    "            print(f'Rate is: {lower}')\n",
    "        \n",
    "        if lower >= 0.5:\n",
    "            delta_max = delta_new\n",
    "        else:\n",
    "            delta = delta_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfx is: [[-2.57477835]]\n",
      "with pred: 0.7032600045204163\n",
      "δ_max using IBP is 0.10900000000000001\n",
      "Minimum concretization required with alpha=0.9999 and R=0.9: 87\n",
      "δ_max using Wilks is 0.11504999999999999\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./models/toy_dnn_enum.h5', compile=False)\n",
    "#model.summary()\n",
    "delta_init = 0.0001\n",
    "\n",
    "cfx = [[-2.57477835]]\n",
    "print('cfx is:', cfx)\n",
    "print('with pred:', model(np.array(cfx)).numpy().item())\n",
    "\n",
    "delta_max_IBP = compute_formal_delta(model, cfx[0], delta_init, use_biases=False)\n",
    "print('δ_max using IBP is', delta_max_IBP)\n",
    "\n",
    "alpha = 0.9999\n",
    "R = 0.9\n",
    "print(f'Minimum concretization required with alpha={alpha} and R={R}:',int(np.emath.logn(R, (1-alpha))))\n",
    "concretizations = 100000\n",
    "delta_max_sampling = compute_delta_max(model, cfx, delta_init, concretizations, use_biases=False)\n",
    "print('δ_max using Wilks is', delta_max_sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write some useful methods..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "\tdef __init__(self, cfx, intervals, percentage=1, last_index_layer=0, last_node_interval=-1, robustness=True, parent=None):\n",
    "\t\t\n",
    "\t\t# Parameters\n",
    "\t\tself.cfx = cfx\n",
    "\t\tself.parent = parent\n",
    "\t\tself.intervals = intervals\n",
    "\t\tself.robustness = robustness\n",
    "\t\tself.percentage = percentage\n",
    "\t\tself.last_index_layer = last_index_layer\n",
    "\t\tself.last_node_interval = last_node_interval\n",
    "\n",
    "\tdef IBP(self): \n",
    "\t\t# create input bounds from CFX for IBP\n",
    "\t\tinput_bounds = []\n",
    "\t\tfor elem in self.cfx[0]:\n",
    "\t\t\tinput_bounds.append(interval([elem, elem]))\n",
    "\n",
    "\t\tpre_activate_bounds = interval(np.maximum(0.0,(self.intervals[1][0]*input_bounds[0])[0]))*self.intervals[2][0] + interval(np.maximum(0.0,(self.intervals[1][1]*input_bounds[0])[0]))*self.intervals[2][1]\n",
    "\t\tlower_bound = 1/(1 + np.exp(-pre_activate_bounds[0][0]))\n",
    "\t\tupper_bound = 1/(1 + np.exp(-pre_activate_bounds[0][1]))\n",
    "\t\n",
    "\t\treturn [lower_bound, upper_bound]\n",
    "\t\n",
    "\tdef split_node(self):\n",
    "\t\tindex_layer =  self.last_index_layer+1 if self.last_index_layer+1 <= 2 else 0\n",
    "\t\tnode_interval = self.last_node_interval+1 if self.last_node_interval+1 <= 1 else 0\n",
    "\t\t\n",
    "\t\tinterval_to_split = self.intervals[index_layer][node_interval]\n",
    "\t\tnew_interval_left = interval([interval_to_split[0][0], interval_to_split.midpoint[0][0]])\n",
    "\t\tnew_interval_right = interval([interval_to_split.midpoint[0][0], interval_to_split[0][1]])\n",
    "\n",
    "\t\tintervals_left = {}\n",
    "\t\tintervals_right = {}\n",
    "\t\tfor key in self.intervals:\n",
    "\t\t\tintervals_left[key] = self.intervals[key].copy()\n",
    "\t\t\tintervals_right[key] = self.intervals[key].copy()\n",
    "\t\t\n",
    "\t\tnew_list_left = intervals_left[index_layer]\n",
    "\t\tnew_list_left[node_interval]= new_interval_left\n",
    "\n",
    "\n",
    "\t\tnew_list_right = intervals_right[index_layer]\n",
    "\t\tnew_list_right[node_interval]= new_interval_right\n",
    "\t\n",
    "\t\tintervals_left[index_layer] = new_list_left\n",
    "\t\tintervals_right[index_layer]= new_list_right\n",
    "\n",
    "\t\treturn [Node(self.cfx, intervals_left, percentage=self.percentage/2, last_index_layer=self.last_index_layer, last_node_interval=self.last_node_interval, robustness=self.robustness, parent=self), Node(self.cfx, intervals_right, percentage=self.percentage/2, last_index_layer=self.last_index_layer, last_node_interval=self.last_node_interval, robustness=self.robustness, parent=self)] \n",
    "\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the INN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./models/toy_dnn_enum.h5', compile=False)\n",
    "delta_max =  0.115\n",
    "cfx = [[-2.57477835]]\n",
    "\n",
    "# Store initial weights\n",
    "old_weights = {}\n",
    "for l in range(1,len(model.layers)):\n",
    "    old_weights[l] = model.layers[l].get_weights()\n",
    "\n",
    "layer_weights_intervals = {}\n",
    "for layer in range(1,len(old_weights)+1):\n",
    "    layer_weights = old_weights[layer][0]\n",
    "   \n",
    "    # every list in layer_weights is the list ok weights that exit from the i-th node of the previous layer. For instance if layer_weights is a list of 5 lists where each of the five list has 7 element, this means that the previous layer has 5 nodes, the next layer has 7 nodes, and the weights that start from the first node of the previous layer to the all the seven nodes of the next layer are stored in the first of the five lists.\n",
    "    # we have to create for each weight an interval and replace the scalar weight with the interval.\n",
    "    layer_intervals = []\n",
    "    for weights_starting_from_node in layer_weights:\n",
    "        for weight in weights_starting_from_node:\n",
    "            weight_interval = interval([weight-delta_max, weight+delta_max])\n",
    "            layer_intervals.append(weight_interval)\n",
    "        \n",
    "        layer_weights_intervals[layer] = layer_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [interval([-0.480079402923584, -0.250079402923584]), interval([-0.9888380074501038, -0.7588380074501038])], 2: [interval([-1.13864981174469, -0.90864981174469]), interval([0.6961773729324341, 0.9261773729324341])]}\n"
     ]
    }
   ],
   "source": [
    "print(layer_weights_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the enumeration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________\n",
      "\n",
      "\u001b[93m[Depth reached]:\u001b[0m 1/40\n",
      "\t\u001b[94m[Monitor]\u001b[0m % robust: 0\n",
      "\t\u001b[94m[Monitor]\u001b[0m % non-robust: 0\n",
      "\t\u001b[94m[Monitor]\u001b[0m Nodes to explore: 2\n",
      "\t\u001b[94m[Monitor]\u001b[0m Maximum % non-robust achievable is: 1.0\n",
      "\t\u001b[94m[Monitor]\u001b[0m The % of INNs non-robust could be > 1-R?: True\n",
      "__________________________________________________________________\n",
      "\n",
      "\u001b[93m[Depth reached]:\u001b[0m 2/40\n",
      "\t\u001b[94m[Monitor]\u001b[0m % robust: 50.0\n",
      "\t\u001b[94m[Monitor]\u001b[0m % non-robust: 0\n",
      "\t\u001b[94m[Monitor]\u001b[0m Nodes to explore: 2\n",
      "\t\u001b[94m[Monitor]\u001b[0m Maximum % non-robust achievable is: 0.5\n",
      "\t\u001b[94m[Monitor]\u001b[0m The % of INNs non-robust could be > 1-R?: True\n",
      "__________________________________________________________________\n",
      "\n",
      "\u001b[93m[Depth reached]:\u001b[0m 3/40\n",
      "\t\u001b[94m[Monitor]\u001b[0m % robust: 75.0\n",
      "\t\u001b[94m[Monitor]\u001b[0m % non-robust: 0\n",
      "\t\u001b[94m[Monitor]\u001b[0m Nodes to explore: 2\n",
      "\t\u001b[94m[Monitor]\u001b[0m Maximum % non-robust achievable is: 0.25\n",
      "\t\u001b[94m[Monitor]\u001b[0m The % of INNs non-robust could be > 1-R?: True\n",
      "__________________________________________________________________\n",
      "\n",
      "\u001b[93m[Depth reached]:\u001b[0m 4/40\n",
      "\t\u001b[94m[Monitor]\u001b[0m % robust: 87.5\n",
      "\t\u001b[94m[Monitor]\u001b[0m % non-robust: 0\n",
      "\t\u001b[94m[Monitor]\u001b[0m Nodes to explore: 2\n",
      "\t\u001b[94m[Monitor]\u001b[0m Maximum % non-robust achievable is: 0.125\n",
      "\t\u001b[94m[Monitor]\u001b[0m The % of INNs non-robust could be > 1-R?: True\n",
      "__________________________________________________________________\n",
      "\n",
      "\u001b[93m[Depth reached]:\u001b[0m 5/40\n",
      "\t\u001b[94m[Monitor]\u001b[0m % robust: 87.5\n",
      "\t\u001b[94m[Monitor]\u001b[0m % non-robust: 0\n",
      "\t\u001b[94m[Monitor]\u001b[0m Nodes to explore: 4\n",
      "\t\u001b[94m[Monitor]\u001b[0m Maximum % non-robust achievable is: 0.125\n",
      "\t\u001b[94m[Monitor]\u001b[0m The % of INNs non-robust could be > 1-R?: True\n",
      "__________________________________________________________________\n",
      "\n",
      "\u001b[93m[Depth reached]:\u001b[0m 6/40\n",
      "\t\u001b[94m[Monitor]\u001b[0m % robust: 90.625\n",
      "\t\u001b[94m[Monitor]\u001b[0m % non-robust: 0\n",
      "\t\u001b[94m[Monitor]\u001b[0m Nodes to explore: 6\n",
      "\t\u001b[94m[Monitor]\u001b[0m Maximum % non-robust achievable is: 0.09375\n",
      "\t\u001b[94m[Monitor]\u001b[0m The % of INNs non-robust could be > 1-R?: False\n",
      "\t\u001b[92m % robust: 0.90625\n",
      "\t\u001b[92m % non-robust: 0\n"
     ]
    }
   ],
   "source": [
    "starting_INN = Node(cfx, layer_weights_intervals)\n",
    "frontier = [starting_INN]\n",
    "next_frontier = []\n",
    "max_depth = 40\n",
    "percentage_not_robust = 0\n",
    "percentage_robust = 0\n",
    "R = 0.9\n",
    "\n",
    "for depth in range(1,max_depth+1):\n",
    "    for INN in frontier:\n",
    "        lower, upper = INN.IBP()\n",
    "      \n",
    "        if upper < 0.5:\n",
    "            # not robust\n",
    "            percentage_not_robust += INN.percentage\n",
    "           \n",
    "        elif lower >= 0.5:\n",
    "            #robust\n",
    "            percentage_robust += INN.percentage\n",
    "            \n",
    "        else:\n",
    "            INN_1, INN_2 = INN.split_node()   \n",
    "            next_frontier.append(INN_1)\n",
    "            next_frontier.append(INN_2)\n",
    "\n",
    "    # The tree has been completely explored\n",
    "    if frontier == []: break\n",
    "\n",
    "    # Update the frontier with the new unexplored nodes\n",
    "    frontier.clear()\n",
    "    frontier = next_frontier.copy()\n",
    "    next_frontier.clear()\n",
    "\n",
    "    print(\"__________________________________________________________________\\n\")\n",
    "    print( f\"{YELLOW_COL}[Depth reached]:{RESET_COL} {depth}/{max_depth}\")\n",
    "    print( f\"\\t{BLUE_COL}[Monitor]{RESET_COL} % robust: {percentage_robust*100}\")\n",
    "    print( f\"\\t{BLUE_COL}[Monitor]{RESET_COL} % non-robust: {percentage_not_robust*100}\")\n",
    "    print( f\"\\t{BLUE_COL}[Monitor]{RESET_COL} Nodes to explore: {len(frontier)}\")\n",
    "    max_non_robust = (1/(2**(depth))) * len(frontier)\n",
    "    print( f\"\\t{BLUE_COL}[Monitor]{RESET_COL} Maximum % non-robust achievable is: {max_non_robust}\")\n",
    "    print( f\"\\t{BLUE_COL}[Monitor]{RESET_COL} The % of INNs non-robust could be > 1-R?: {max_non_robust > 1-R}\")\n",
    "\n",
    "    if max_non_robust <= 1-R:\n",
    "        break\n",
    "\n",
    "print( f\"\\t{GREEN_COL} % robust: {percentage_robust}\")\n",
    "print( f\"\\t{GREEN_COL} % non-robust: {percentage_not_robust}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planning-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
